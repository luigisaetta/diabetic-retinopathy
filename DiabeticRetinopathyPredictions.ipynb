{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re, math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# now opart of tensorflow\n",
    "# now opart of tensorflow\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove some warnings\n",
    "\n",
    "# TF2 way to reduce logging\n",
    "# this remove also INFO, verify if needed\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "Using strategy for multiple GPU\n",
      "REPLICAS: 2\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'GPU'\n",
    "\n",
    "if DEVICE == \"GPU\":\n",
    "    n_gpu = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print(\"Num GPUs Available: \", n_gpu)\n",
    "    \n",
    "    if n_gpu > 1:\n",
    "        print(\"Using strategy for multiple GPU\")\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        print('Standard strategy for GPU...')\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/ubuntu/kaggle/diabetic'\n",
    "\n",
    "TEST_IMAGES_DIR = os.path.join(BASE_DIR, 'test256')\n",
    "\n",
    "TFREC_DIR = os.path.join(BASE_DIR, 'tfrec')\n",
    "\n",
    "TEST_FILENAMES = tf.io.gfile.glob(os.path.join(TFREC_DIR,'test*.tfrec'))\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imSize = 256\n",
    "IMAGE_SIZE= [256,256]\n",
    "BATCH_SIZE = 512*4\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    # porto a 256x256\n",
    "    # image = tf.image.resize(image, [imSize,imSize])\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        'patient_id' : tf.io.FixedLenFeature([], tf.int64), \n",
    "        'side' : tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    \n",
    "    image = decode_image(example['image'])\n",
    "    patient_id = example['patient_id']\n",
    "    side = example['side']\n",
    "    \n",
    "    return image, patient_id, side\n",
    "\n",
    "def get_test_dataset(filenames):\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.map(read_unlabeled_tfrecord)\n",
    "    \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "# count # of images in files.. (embedded in file name)\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
    "         for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53576"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test_images =  count_data_items(TEST_FILENAMES)\n",
    "\n",
    "n_test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione vettoriale di conversione 0 -> left, 1 -> right\n",
    "def side_map(side):\n",
    "    if side == 0:\n",
    "        s_side = 'left'\n",
    "    else:\n",
    "        s_side = 'right'\n",
    "    return s_side\n",
    "\n",
    "v_side_map = np.vectorize(side_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carico il modello dall'ultimo run\n",
    "# per ora prendo fold0\n",
    "# adding some data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  # introduced in TF 2.3\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.4),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomContrast(0.2)\n",
    "])\n",
    "\n",
    "# here we define the DNN Model\n",
    "\n",
    "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
    "        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n",
    "\n",
    "# as default it used B0\n",
    "\n",
    "def build_model(dim = 256, ef = 0):\n",
    "    inp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n",
    "    \n",
    "    # introdotta la data augmentation come parte del modello\n",
    "    x = data_augmentation(inp)\n",
    "    \n",
    "    base = EFNS[ef](input_shape=(*IMAGE_SIZE, 3), weights='imagenet', include_top = False)\n",
    "    \n",
    "    x = base(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = inp,outputs = x)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    # loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0) \n",
    "    \n",
    "    model.compile(optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: reading TFREcords file...\n",
      "Examples of the test data:\n",
      "Predictions using model fold  0\n",
      "Predictions using model fold  1\n",
      "Predictions using model fold  2\n",
      "Predictions using model fold  3\n",
      "Predictions using model fold  4\n"
     ]
    }
   ],
   "source": [
    "# in questo modo itero una volta sull'intero test dataset\n",
    "# ed estraggo tutti i nomi dei file delle immagini\n",
    "# TODO poi batch per batch posso effettuare le predizioni\n",
    "\n",
    "print('Test: reading TFREcords file...')\n",
    "print(\"Examples of the test data:\")\n",
    "\n",
    "\n",
    "class_prob_predictions = np.zeros(shape = (FOLDS, n_test_images, NUM_CLASSES))\n",
    "\n",
    "for fold in range(0,5):\n",
    "    # ricarica i dati\n",
    "    test_dataset = get_test_dataset(TEST_FILENAMES)\n",
    "    \n",
    "    print('Predictions using model fold ', fold)\n",
    "    \n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = build_model(ef = 4)\n",
    "\n",
    "    model.load_weights('fold-%i.h5'%fold)\n",
    "    \n",
    "    batch_num = 0\n",
    "    img_names = np.array([])\n",
    "    class_predictions_fold = np.zeros(shape = (n_test_images, NUM_CLASSES))\n",
    "    \n",
    "    for image, patient_id, side in iter(test_dataset):\n",
    "        # print('Working on batch num: ', batch_num)\n",
    "    \n",
    "        # predizione sul singolo batch\n",
    "        preds_batch = model.predict(image.numpy())\n",
    "        \n",
    "        # devo costruire i nomi dei files\n",
    "        s_patient_id = np.char.mod('%d', patient_id.numpy())\n",
    "        name = np.char.add(s_patient_id, '_')\n",
    "    \n",
    "        # side devo trasformare 0 in left, 1 in right\n",
    "        s_side = v_side_map(side.numpy())\n",
    "    \n",
    "        name = np.char.add(name, s_side)\n",
    "    \n",
    "        # salvo \n",
    "        img_names = np.append(img_names, name)\n",
    "        \n",
    "        start = batch_num * len(preds_batch)\n",
    "        end = ((batch_num + 1) * len(preds_batch))\n",
    "        class_predictions_fold[start:end] = preds_batch\n",
    "        \n",
    "        batch_num += 1\n",
    "    \n",
    "    # end of fold\n",
    "    class_prob_predictions[fold] = class_predictions_fold\n",
    "\n",
    "# alla fine    \n",
    "# faccio la media sugli n folds\n",
    "class_prob_predictions_avg = np.mean(class_prob_predictions, axis = 0)\n",
    "\n",
    "classes_predicted = np.argmax(class_prob_predictions_avg, axis = 1)\n",
    "\n",
    "# converto ad interi\n",
    "classes_predicted = classes_predicted.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53576"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53576"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ora devo combinare image name e class_predictions in un singolo file\n",
    "submission = pd.DataFrame(dict(image=img_names, level=classes_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53556</th>\n",
       "      <td>16271_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53557</th>\n",
       "      <td>19259_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53558</th>\n",
       "      <td>8184_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53559</th>\n",
       "      <td>33609_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53560</th>\n",
       "      <td>43992_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53561</th>\n",
       "      <td>38085_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53562</th>\n",
       "      <td>4348_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53563</th>\n",
       "      <td>5170_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53564</th>\n",
       "      <td>17765_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53565</th>\n",
       "      <td>33982_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53566</th>\n",
       "      <td>43428_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53567</th>\n",
       "      <td>9974_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53568</th>\n",
       "      <td>33579_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53569</th>\n",
       "      <td>4575_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53570</th>\n",
       "      <td>19397_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53571</th>\n",
       "      <td>27835_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53572</th>\n",
       "      <td>40281_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53573</th>\n",
       "      <td>10877_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53574</th>\n",
       "      <td>39806_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53575</th>\n",
       "      <td>6797_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image  level\n",
       "53556  16271_right      0\n",
       "53557  19259_right      0\n",
       "53558    8184_left      0\n",
       "53559  33609_right      0\n",
       "53560  43992_right      0\n",
       "53561   38085_left      0\n",
       "53562    4348_left      0\n",
       "53563    5170_left      0\n",
       "53564   17765_left      0\n",
       "53565   33982_left      0\n",
       "53566  43428_right      0\n",
       "53567   9974_right      0\n",
       "53568  33579_right      0\n",
       "53569    4575_left      0\n",
       "53570  19397_right      0\n",
       "53571  27835_right      0\n",
       "53572  40281_right      0\n",
       "53573  10877_right      0\n",
       "53574  39806_right      0\n",
       "53575    6797_left      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.sort_values('image') \n",
    "submission.to_csv('sub_diabetic08.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KAGGLE_USERNAME=luigisaetta\n",
      "env: KAGGLE_KEY=8052f0a7efb83795d3da6ef4eb47dc5c\n",
      "100%|█████████████████████████████████████████| 693k/693k [00:02<00:00, 251kB/s]\n",
      "Successfully submitted to Diabetic Retinopathy Detection"
     ]
    }
   ],
   "source": [
    "# kaggle submission\n",
    "%env KAGGLE_USERNAME=luigisaetta\n",
    "%env KAGGLE_KEY = 8052f0a7efb83795d3da6ef4eb47dc5c\n",
    "\n",
    "! kaggle competitions submit -c diabetic-retinopathy-detection -f sub_diabetic08.csv -m \"5 fold, old ef4, gaussian blur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission.level.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
